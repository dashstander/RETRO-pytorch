{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c75571d-0231-4165-94fb-2265f9504a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/home/dashiell/.cache/huggingface/datasets/wikipedia/20200501.en/1.0.0/009f923d9b6dd00c00c8cdc7f408f2b47f45dd4f5fb7982a21f9448f4afbe475)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7084249700004fa9bdf45ce1772aac93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from einops import rearrange\n",
    "import faiss\n",
    "from autofaiss import build_index\n",
    "\n",
    "dataset = load_dataset(\"wikipedia\", \"20200501.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7158dd56-f182-4742-99ab-f66a39397e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6078422"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb63cd7-a2e5-4933-a1f6-b15a328bc2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Nickel (disambiguation)', 'text': 'Nickel is a chemical element.\\n\\nNickel may also refer to:\\n\\nPeople\\n Nickel (surname)\\n Nickel Ashmeade (born 1990), Jamaican athlete\\n Nickel Chand (born 1995), Fijian footballer\\n Nickel Hoffmann (1536–1592), German stonemason\\n Nickel Leung, Hong Kong educator\\n\\nCoins and tokens\\n Nickel (Canadian coin), a five cent coin introduced in 1922\\n Nickel (United States coin), a five cent coin introduced in 1866\\n Half dime, a U.S. five cent coin produced in various years in the range 1792–1873 (sometimes called a \"nickel\" due to its face value)\\n Three-cent nickel, a U.S. coin (1865–1889)\\n Indian Head cent, a U.S. coin (1859–1864) nicknamed the \"nickel\"\\n\\nGames and sports\\n Nickel defense, a defense formation in American and Canadian football\\n Nickel Trophy, awarded to the winner of the football game between North Dakota State University and the University of North Dakota\\n\\nOther uses\\n Nickel, a shade of gray\\n Nickel Theatre, St. John\\'s, Newfoundland, Canada\\n Nickel Film Festival, St. John\\'s, Newfoundland\\n\\nSee also\\n\\n Nickle (disambiguation)\\n Nickels (disambiguation)\\n Nichol, a surname\\n Nichols (disambiguation)\\n Nicole (disambiguation)'}\n"
     ]
    }
   ],
   "source": [
    "doc_text = dataset[934590]\n",
    "chunk_size = 64\n",
    "seq_len = 2048\n",
    "pad_id = 0\n",
    "print(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9e1c8b-67e2-44bb-bd0b-81214e960389",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL = None\n",
    "TOKENIZER = None\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def range_chunked(max_value, *, batch_size):\n",
    "    counter = 0\n",
    "    while counter < max_value:\n",
    "        curr = counter + batch_size\n",
    "        curr = min(curr, max_value)\n",
    "        yield slice(counter, curr)\n",
    "        counter = curr\n",
    "\n",
    "# indexing helper functions\n",
    "\n",
    "def faiss_read_index(path):\n",
    "    return faiss.read_index(str(path), faiss.IO_FLAG_MMAP | faiss.IO_FLAG_READ_ONLY)\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def get_tokenizer():\n",
    "    global TOKENIZER\n",
    "    if not exists(TOKENIZER):\n",
    "        TOKENIZER = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    return TOKENIZER\n",
    "\n",
    "def get_bert():\n",
    "    global MODEL\n",
    "    if not exists(MODEL):\n",
    "        MODEL = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "        if torch.cuda.is_available():\n",
    "            MODEL = MODEL.cuda()\n",
    "    return MODEL\n",
    "\n",
    "def tokenize(texts, add_special_tokens = True):\n",
    "    if not isinstance(texts, (list, tuple)):\n",
    "        texts = [texts]\n",
    "\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens = add_special_tokens,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    token_ids = encoding.input_ids\n",
    "    return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c776c07-6171-4f24-bbb8-178d5f9db1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 15523,  2007,  1041,  5076,  5787,  1016, 15523,  2093,  2040,\n",
      "          6527,  2004,  1028,  2115, 15523,  1010, 11992,  1011, 15523,  6687,\n",
      "          4172,  9652,  1010,  2145,  2905,  1011,  1014, 17855,  8262, 15523,\n",
      "          9216,  2098,  1010,  2145,  2790,  1011,  1014, 11468,  2323,  4366,\n",
      "         15523, 24441,  1010, 16714,  2579,  1520, 18918,  2479,  1011,  1014,\n",
      "          2450,  2966,  9339,  2243, 15523, 26041,  1014,  4295,  4294, 11494,\n",
      "          7828,  2002, 19208,  2019, 15523,  1010,  3014,  9230,  1011,  1014,\n",
      "          1041,  2278,  9362,  9230,  3111,  2003,  4802, 15523,  1010,  2146,\n",
      "          2167,  9230,  1011,  1014,  1041,  2278,  9362,  9230,  3111,  2003,\n",
      "          7651,  2435, 27215,  1014,  1041,  1061,  1016,  1059,  1016,  2278,\n",
      "          9362,  9230,  2554,  2003,  2540,  2090,  2003,  2000,  2850, 13418,\n",
      "          1520,  7616,  1010,  2827,  2174,  1041,  1004, 15523,  1004,  2353,\n",
      "          2004,  2053,  2231,  3647,  1011,  2097,  1015,  9362, 15523,  1014,\n",
      "          1041,  1061,  1016,  1059,  1016,  9230,  1010,  6729,  1520,  6482,\n",
      "          1011,  2800,  2136,  9362,  1014,  1041,  1061,  1016,  1059,  1016,\n",
      "          9230,  1010,  8169,  1520,  6721,  1011,  9923,  2000,  1004, 15523,\n",
      "          1004,  2403,  2002,  3002, 15523,  3643,  1014,  1041,  3643,  4199,\n",
      "          2003,  2141,  2002,  3014,  2378, 15523,  5388,  1014,  3022,  2004,\n",
      "          2000,  3457,  2001,  2000,  2378,  2212,  2094,  2171,  7738,  2114,\n",
      "          2122,  2002,  2000,  2122,  2001,  2171,  7738,  2064,  3598, 15523,\n",
      "          1014,  1041,  8707,  2001,  3901, 15523,  3008,  1014,  2362,  1016,\n",
      "          2202,  1009,  1059,  1014, 11948,  1014,  2714, 15523,  2147,  2786,\n",
      "          1014,  2362,  1016,  2202,  1009,  1059,  1014, 11948,  2160,  2040,\n",
      "          4176,  2575,  1010,  4491, 21563,  5642, 19700,  3512,  1011, 15523,\n",
      "          2019,  1010,  4491, 21563,  5642, 19700,  3512,  1011, 27973, 14858,\n",
      "          1014,  1041, 11992, 15750,  1010,  4491, 21563,  5642, 19700,  3512,\n",
      "          1011,  9855,  1010,  4491, 21563,  5642, 19700,  3512,  1011,     2]])\n",
      "torch.Size([1, 270])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "assert (seq_len % chunk_size) == 0, 'sequence length must be divisible by chunk size'\n",
    "\n",
    "ids = tokenize(doc_text)\n",
    "print(ids)\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa62215-1b2e-4446-9ecf-eab9f74c805f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0, 15523,  2007,  1041,  5076,  5787,  1016, 15523,  2093,  2040,\n",
      "         6527,  2004,  1028,  2115, 15523,  1010, 11992,  1011, 15523,  6687,\n",
      "         4172,  9652,  1010,  2145,  2905,  1011,  1014, 17855,  8262, 15523,\n",
      "         9216,  2098,  1010,  2145,  2790,  1011,  1014, 11468,  2323,  4366,\n",
      "        15523, 24441,  1010, 16714,  2579,  1520, 18918,  2479,  1011,  1014,\n",
      "         2450,  2966,  9339,  2243, 15523, 26041,  1014,  4295,  4294, 11494,\n",
      "         7828,  2002, 19208,  2019, 15523,  1010,  3014,  9230,  1011,  1014,\n",
      "         1041,  2278,  9362,  9230,  3111,  2003,  4802, 15523,  1010,  2146,\n",
      "         2167,  9230,  1011,  1014,  1041,  2278,  9362,  9230,  3111,  2003,\n",
      "         7651,  2435, 27215,  1014,  1041,  1061,  1016,  1059,  1016,  2278,\n",
      "         9362,  9230,  2554,  2003,  2540,  2090,  2003,  2000,  2850, 13418,\n",
      "         1520,  7616,  1010,  2827,  2174,  1041,  1004, 15523,  1004,  2353,\n",
      "         2004,  2053,  2231,  3647,  1011,  2097,  1015,  9362, 15523,  1014,\n",
      "         1041,  1061,  1016,  1059,  1016,  9230,  1010,  6729,  1520,  6482,\n",
      "         1011,  2800,  2136,  9362,  1014,  1041,  1061,  1016,  1059,  1016,\n",
      "         9230,  1010,  8169,  1520,  6721,  1011,  9923,  2000,  1004, 15523,\n",
      "         1004,  2403,  2002,  3002, 15523,  3643,  1014,  1041,  3643,  4199,\n",
      "         2003,  2141,  2002,  3014,  2378, 15523,  5388,  1014,  3022,  2004,\n",
      "         2000,  3457,  2001,  2000,  2378,  2212,  2094,  2171,  7738,  2114,\n",
      "         2122,  2002,  2000,  2122,  2001,  2171,  7738,  2064,  3598, 15523,\n",
      "         1014,  1041,  8707,  2001,  3901, 15523,  3008,  1014,  2362,  1016,\n",
      "         2202,  1009,  1059,  1014, 11948,  1014,  2714, 15523,  2147,  2786,\n",
      "         1014,  2362,  1016,  2202,  1009,  1059,  1014, 11948,  2160,  2040,\n",
      "         4176,  2575,  1010,  4491, 21563,  5642, 19700,  3512,  1011, 15523,\n",
      "         2019,  1010,  4491, 21563,  5642, 19700,  3512,  1011, 27973, 14858,\n",
      "         1014,  1041, 11992, 15750,  1010,  4491, 21563,  5642, 19700,  3512,\n",
      "         1011,  9855,  1010,  4491, 21563,  5642, 19700,  3512,  1011,     2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([270])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = rearrange(ids, '1 ... -> ...')\n",
    "\n",
    "print(ids)\n",
    "ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5e74781-aff1-4a72-ae7d-d27a8db50e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0, 15523,  2007,  1041,  5076,  5787,  1016, 15523,  2093,  2040,\n",
       "         6527,  2004,  1028,  2115, 15523,  1010, 11992,  1011, 15523,  6687,\n",
       "         4172,  9652,  1010,  2145,  2905,  1011,  1014, 17855,  8262, 15523,\n",
       "         9216,  2098,  1010,  2145,  2790,  1011,  1014, 11468,  2323,  4366,\n",
       "        15523, 24441,  1010, 16714,  2579,  1520, 18918,  2479,  1011,  1014,\n",
       "         2450,  2966,  9339,  2243, 15523, 26041,  1014,  4295,  4294, 11494,\n",
       "         7828,  2002, 19208,  2019, 15523,  1010,  3014,  9230,  1011,  1014,\n",
       "         1041,  2278,  9362,  9230,  3111,  2003,  4802, 15523,  1010,  2146,\n",
       "         2167,  9230,  1011,  1014,  1041,  2278,  9362,  9230,  3111,  2003,\n",
       "         7651,  2435, 27215,  1014,  1041,  1061,  1016,  1059,  1016,  2278,\n",
       "         9362,  9230,  2554,  2003,  2540,  2090,  2003,  2000,  2850, 13418,\n",
       "         1520,  7616,  1010,  2827,  2174,  1041,  1004, 15523,  1004,  2353,\n",
       "         2004,  2053,  2231,  3647,  1011,  2097,  1015,  9362, 15523,  1014,\n",
       "         1041,  1061,  1016,  1059,  1016,  9230,  1010,  6729,  1520,  6482,\n",
       "         1011,  2800,  2136,  9362,  1014,  1041,  1061,  1016,  1059,  1016,\n",
       "         9230,  1010,  8169,  1520,  6721,  1011,  9923,  2000,  1004, 15523,\n",
       "         1004,  2403,  2002,  3002, 15523,  3643,  1014,  1041,  3643,  4199,\n",
       "         2003,  2141,  2002,  3014,  2378, 15523,  5388,  1014,  3022,  2004,\n",
       "         2000,  3457,  2001,  2000,  2378,  2212,  2094,  2171,  7738,  2114,\n",
       "         2122,  2002,  2000,  2122,  2001,  2171,  7738,  2064,  3598, 15523,\n",
       "         1014,  1041,  8707,  2001,  3901, 15523,  3008,  1014,  2362,  1016,\n",
       "         2202,  1009,  1059,  1014, 11948,  1014,  2714, 15523,  2147,  2786,\n",
       "         1014,  2362,  1016,  2202,  1009,  1059,  1014, 11948,  2160,  2040,\n",
       "         4176,  2575,  1010,  4491, 21563,  5642, 19700,  3512,  1011, 15523,\n",
       "         2019,  1010,  4491, 21563,  5642, 19700,  3512,  1011, 27973, 14858,\n",
       "         1014,  1041, 11992, 15750,  1010,  4491, 21563,  5642, 19700,  3512,\n",
       "         1011,  9855,  1010,  4491, 21563,  5642, 19700,  3512,  1011,     2,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_len = ids.shape[-1]\n",
    "\n",
    "# pad to multiple of chunk size with an extra token\n",
    "\n",
    "padding = chunk_size - ((text_len - 1) % chunk_size)\n",
    "ids = F.pad(ids, (0, padding))\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a245dc42-eeb6-4584-8519-21c09571064a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 15523,  2007,  1041,  5076,  5787,  1016, 15523,  2093,  2040,\n",
      "          6527,  2004,  1028,  2115, 15523,  1010, 11992,  1011, 15523,  6687,\n",
      "          4172,  9652,  1010,  2145,  2905,  1011,  1014, 17855,  8262, 15523,\n",
      "          9216,  2098,  1010,  2145,  2790,  1011,  1014, 11468,  2323,  4366,\n",
      "         15523, 24441,  1010, 16714,  2579,  1520, 18918,  2479,  1011,  1014,\n",
      "          2450,  2966,  9339,  2243, 15523, 26041,  1014,  4295,  4294, 11494,\n",
      "          7828,  2002, 19208,  2019],\n",
      "        [15523,  1010,  3014,  9230,  1011,  1014,  1041,  2278,  9362,  9230,\n",
      "          3111,  2003,  4802, 15523,  1010,  2146,  2167,  9230,  1011,  1014,\n",
      "          1041,  2278,  9362,  9230,  3111,  2003,  7651,  2435, 27215,  1014,\n",
      "          1041,  1061,  1016,  1059,  1016,  2278,  9362,  9230,  2554,  2003,\n",
      "          2540,  2090,  2003,  2000,  2850, 13418,  1520,  7616,  1010,  2827,\n",
      "          2174,  1041,  1004, 15523,  1004,  2353,  2004,  2053,  2231,  3647,\n",
      "          1011,  2097,  1015,  9362],\n",
      "        [15523,  1014,  1041,  1061,  1016,  1059,  1016,  9230,  1010,  6729,\n",
      "          1520,  6482,  1011,  2800,  2136,  9362,  1014,  1041,  1061,  1016,\n",
      "          1059,  1016,  9230,  1010,  8169,  1520,  6721,  1011,  9923,  2000,\n",
      "          1004, 15523,  1004,  2403,  2002,  3002, 15523,  3643,  1014,  1041,\n",
      "          3643,  4199,  2003,  2141,  2002,  3014,  2378, 15523,  5388,  1014,\n",
      "          3022,  2004,  2000,  3457,  2001,  2000,  2378,  2212,  2094,  2171,\n",
      "          7738,  2114,  2122,  2002],\n",
      "        [ 2000,  2122,  2001,  2171,  7738,  2064,  3598, 15523,  1014,  1041,\n",
      "          8707,  2001,  3901, 15523,  3008,  1014,  2362,  1016,  2202,  1009,\n",
      "          1059,  1014, 11948,  1014,  2714, 15523,  2147,  2786,  1014,  2362,\n",
      "          1016,  2202,  1009,  1059,  1014, 11948,  2160,  2040,  4176,  2575,\n",
      "          1010,  4491, 21563,  5642, 19700,  3512,  1011, 15523,  2019,  1010,\n",
      "          4491, 21563,  5642, 19700,  3512,  1011, 27973, 14858,  1014,  1041,\n",
      "         11992, 15750,  1010,  4491],\n",
      "        [21563,  5642, 19700,  3512,  1011,  9855,  1010,  4491, 21563,  5642,\n",
      "         19700,  3512,  1011,     2,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "ids, last_token = ids[:-1], ids[-1:]\n",
    "ids = rearrange(ids, '(n c) -> n c', c = chunk_size)\n",
    "print(ids)\n",
    "print(last_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1adbf63-751f-461b-a273-1f790de474c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15523, 15523,  2000, 21563])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_per_chunk = ids[1:, 0]\n",
    "last_token_per_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "40b1cd32-61bb-48e4-ab92-289b2067199a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15523],\n",
       "        [15523],\n",
       "        [ 2000],\n",
       "        [21563],\n",
       "        [    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_last_tokens = torch.cat((last_token_per_chunk, last_token), dim = 0)\n",
    "\n",
    "all_last_tokens = rearrange(all_last_tokens, 'n -> n 1')\n",
    "all_last_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9d0f981-6833-4c1d-bd99-1f7c3fe70372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 65])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append all last tokens to ids for (num_chunks, chunk_size + 1)\n",
    "chunks_with_extra_token = torch.cat((ids, all_last_tokens), dim = -1)\n",
    "chunks_with_extra_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfd3d9a7-264e-4ac6-b776-3b887e611a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_chunks = ids.shape[0]\n",
    "num_chunks_per_seq = seq_len // chunk_size\n",
    "num_chunks_per_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5328d79d-aabf-4226-bc0a-2f8815e2fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "MODEL = None\n",
    "TOKENIZER = None\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "def get_tokenizer():\n",
    "    global TOKENIZER\n",
    "    if not exists(TOKENIZER):\n",
    "        TOKENIZER = AutoTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "    return TOKENIZER\n",
    "\n",
    "def get_bert():\n",
    "    global MODEL\n",
    "    if not exists(MODEL):\n",
    "        MODEL = AutoModel.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "        if torch.cuda.is_available():\n",
    "            MODEL = MODEL.cuda()\n",
    "    return MODEL\n",
    "\n",
    "# tokenize\n",
    "\n",
    "def tokenize(texts, add_special_tokens = True):\n",
    "    if not isinstance(texts, (list, tuple)):\n",
    "        texts = [texts]\n",
    "\n",
    "    tokenizer = get_tokenizer()\n",
    "\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens = add_special_tokens,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    token_ids = encoding.input_ids\n",
    "    return token_ids\n",
    "\n",
    "# text to chunks\n",
    "\n",
    "def doc_text_to_chunks_and_seq_indices(\n",
    "    *,\n",
    "    doc_text,\n",
    "    chunk_size = 64,\n",
    "    seq_len = 2048,\n",
    "    pad_id = 0\n",
    "):\n",
    "    assert (seq_len % chunk_size) == 0, 'sequence length must be divisible by chunk size'\n",
    "\n",
    "    ids = tokenize(doc_text)\n",
    "    ids = rearrange(ids, '1 ... -> ...')\n",
    "\n",
    "    text_len = ids.shape[-1]\n",
    "\n",
    "    # pad to multiple of chunk size with an extra token\n",
    "\n",
    "    padding = chunk_size - ((text_len - 1) % chunk_size)\n",
    "    ids = F.pad(ids, (0, padding))\n",
    "\n",
    "    # split out very last token\n",
    "\n",
    "    ids, last_token = ids[:-1], ids[-1:]\n",
    "    ids = rearrange(ids, '(n c) -> n c', c = chunk_size)\n",
    "\n",
    "    # first tokens of chunk [2:] and on will become the last token of chunk [1:]\n",
    "\n",
    "    last_token_per_chunk = ids[1:, 0]\n",
    "    all_last_tokens = torch.cat((last_token_per_chunk, last_token), dim = 0)\n",
    "    all_last_tokens = rearrange(all_last_tokens, 'n -> n 1') # Transpose sorta? [x, y, z] -> [[x], [y], [z]]\n",
    "\n",
    "    # append all last tokens to ids for (num_chunks, chunk_size + 1)\n",
    "\n",
    "    chunks_with_extra_token = torch.cat((ids, all_last_tokens), dim = -1)\n",
    "\n",
    "    # calculate chunk indices starting at 0, spaced number of chunks of seq len apart\n",
    "\n",
    "    total_chunks = ids.shape[0]\n",
    "    num_chunks_per_seq = seq_len // chunk_size\n",
    "    seq = torch.arange(0, total_chunks, num_chunks_per_seq)\n",
    "    return chunks_with_extra_token, seq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5ce764-318f-4f50-9757-d65cf8bd0af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8f95a6-4c95-451c-9689-c80fac9547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def text_dataset_to_chunks_(\n",
    "    *,\n",
    "    dataset,\n",
    "    chunks_memmap_path,\n",
    "    seqs_memmap_path,\n",
    "    doc_ids_memmap_path,\n",
    "    chunk_size = 64,\n",
    "    seq_len = 2048,\n",
    "    glob = '**/*.txt',\n",
    "    max_chunks = 1_00_000_000,\n",
    "    max_seqs = 1_00_000\n",
    "):\n",
    "\n",
    "    total_chunks = 0\n",
    "    total_docs = 0\n",
    "    total_seqs = 0\n",
    "\n",
    "    chunks_shape = (max_chunks, chunk_size + 1)\n",
    "    seqs_shape = (max_seqs,)\n",
    "    doc_ids_shape = (max_chunks,)\n",
    "    seq_stack = []\n",
    "    chunk_stack = []\n",
    "\n",
    "    with (\n",
    "        # huge on-disk numpy array of shape (VERY_BIG, 65), writes a set of length-65 rows which made up of token IDs\n",
    "        memmap(chunks_memmap_path, shape = chunks_shape, dtype = np.int32, mode = 'w+') as chunks_memmap,\n",
    "        # \n",
    "        memmap(seqs_memmap_path, shape = seqs_shape, dtype = np.int32, mode = 'w+') as seqs_memmap,\n",
    "        memmap(doc_ids_memmap_path, shape = doc_ids_shape, dtype = np.int32, mode = 'w+') as doc_ids_memmap\n",
    "    ):\n",
    "        for text in tqdm(dataset):\n",
    "            chunks, seq = doc_text_to_chunks_and_seq_indices(\n",
    "                doc_text = text['text'],\n",
    "                chunk_size = chunk_size,\n",
    "                seq_len = seq_len\n",
    "            )\n",
    "            \"\"\"\n",
    "            chunk_stack.append(chunks)\n",
    "            seq_stack.append(seq)\n",
    "            if len(chunk_stack) > 50:\n",
    "                chunk_stack.pop(0)\n",
    "                seq_stack.pop(0)\n",
    "            \"\"\"\n",
    "            doc_chunk_len = chunks.shape[0]\n",
    "            doc_seq_len = seq.shape[0]\n",
    "            chunks_memmap[total_chunks:(total_chunks + doc_chunk_len)] = chunks.numpy()\n",
    "            try:\n",
    "                seqs_memmap[total_seqs:(total_seqs + doc_seq_len)] = seq.numpy() + total_chunks\n",
    "            except ValueError as ve:\n",
    "                for chunk_hist, seq_hist in zip(chunk_stack, seq_stack):\n",
    "                    print(seq_hist, chunk_hist)\n",
    "                raise ve\n",
    "            doc_ids_memmap[total_chunks:(total_chunks + doc_chunk_len)] = np.full((doc_chunk_len,), total_docs)\n",
    "            last_seq = seq\n",
    "            total_chunks += doc_chunk_len\n",
    "            total_seqs += doc_seq_len\n",
    "            total_docs += 1\n",
    "\n",
    "    return dict(\n",
    "        chunks = total_chunks,\n",
    "        docs = total_docs,\n",
    "        seqs = total_seqs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "342447f3-a8cf-46f0-894f-eaf223b0b787",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def bert_embed(\n",
    "    token_ids,\n",
    "    return_cls_repr = False,\n",
    "    eps = 1e-8,\n",
    "    pad_id = 0.\n",
    "):\n",
    "    model = get_bert()\n",
    "    mask = token_ids != pad_id\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        token_ids = token_ids.cuda()\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids = token_ids,\n",
    "        attention_mask = mask,\n",
    "        output_hidden_states = True\n",
    "    )\n",
    "\n",
    "    hidden_state = outputs.hidden_states[-1]\n",
    "\n",
    "    if return_cls_repr:\n",
    "        return hidden_state[:, 0]               # return [cls] as representation\n",
    "\n",
    "    if not exists(mask):\n",
    "        return hidden_state.mean(dim = 1)\n",
    "\n",
    "    mask = mask[:, 1:]                          # mean all tokens excluding [cls], accounting for length\n",
    "    mask = rearrange(mask, 'b n -> b n 1')\n",
    "\n",
    "    numer = (hidden_state[:, 1:] * mask).sum(dim = 1)\n",
    "    denom = mask.sum(dim = 1)\n",
    "    masked_mean =  numer / (denom + eps)\n",
    "    return masked_mean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49142f2e-65ca-4590-9079-f974fc94a95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af38fc128049401ca02ac8e3f1b30c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6078422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (730 > 512). Running this sequence through the model will result in indexing errors\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from retro_pytorch.utils import memmap, reset_folder_\n",
    "\n",
    "chunk_size = 64\n",
    "knn = 2\n",
    "seq_len = 2048\n",
    "chunks_memmap_path = './train.chunks.dat'\n",
    "seqs_memmap_path = './train.seq.dat'\n",
    "doc_ids_memmap_path = './train.doc_ids.dat'\n",
    "max_chunks = 1_000_000_000\n",
    "max_seqs = 100_000_000\n",
    "knn_extra_neighbors = 100\n",
    "processed_stats_json_path = './processed-stats.json'\n",
    "faiss_index_filename = 'knn.index'\n",
    "\n",
    "\n",
    "# constants\n",
    "\n",
    "SOS_ID = 101\n",
    "EOS_ID = 102\n",
    "BERT_MODEL_DIM = 768\n",
    "BERT_VOCAB_SIZE = 28996\n",
    "\n",
    "TMP_PATH = Path('./.tmp')\n",
    "INDEX_FOLDER_PATH = TMP_PATH / '.index'\n",
    "EMBEDDING_TMP_SUBFOLDER = 'embeddings'\n",
    "\n",
    "# helper functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def range_chunked(max_value, *, batch_size):\n",
    "    counter = 0\n",
    "    while counter < max_value:\n",
    "        curr = counter + batch_size\n",
    "        curr = min(curr, max_value)\n",
    "        yield slice(counter, curr)\n",
    "        counter = curr\n",
    "\n",
    "# indexing helper functions\n",
    "\n",
    "def faiss_read_index(path):\n",
    "    return faiss.read_index(str(path), faiss.IO_FLAG_MMAP | faiss.IO_FLAG_READ_ONLY)\n",
    "\n",
    "\n",
    "stats = text_dataset_to_chunks_(\n",
    "    dataset = dataset, \n",
    "    chunks_memmap_path = chunks_memmap_path,\n",
    "    seqs_memmap_path = seqs_memmap_path,\n",
    "    doc_ids_memmap_path = doc_ids_memmap_path,\n",
    "    chunk_size = chunk_size,\n",
    "    seq_len = seq_len,\n",
    "    max_chunks = max_chunks,\n",
    "    max_seqs = max_seqs\n",
    ")\n",
    "\n",
    "\n",
    "num_chunks = stats['chunks']\n",
    "num_seqs = stats['seqs']\n",
    "\n",
    "# calculate knn memmap path and get the faiss index\n",
    "# todo - make sure if faiss_index_filename is found, do not reprocess unless flag is given\n",
    "\n",
    "knn_memmap_path, faiss_index = chunks_to_precalculated_knn_(\n",
    "    num_chunks = num_chunks,\n",
    "    chunk_size = chunk_size,\n",
    "    chunk_memmap_path = chunks_memmap_path,\n",
    "    doc_ids_memmap_path = doc_ids_memmap_path,\n",
    "    num_nearest_neighbors = knn,\n",
    "    num_extra_neighbors = knn_extra_neighbors,\n",
    "    index_file = faiss_index_filename,\n",
    "    force_reprocess = force_reprocess,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc66a54-32e1-494c-a056-3ab6ee98e727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:retro]",
   "language": "python",
   "name": "conda-env-retro-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
